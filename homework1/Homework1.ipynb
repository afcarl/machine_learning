{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 (5% credit) \n",
    "Provide a definition for the concept of a conjugate prior given the Bayesian learning model. What is the conjugate prior for the univariate linear regression $y\\sim {\\cal N}(wx,\\sigma^2)$ with a fixed $\\sigma$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A conjugate prior is a prior that has is from the same family of distributions as the postierier.\n",
    "\n",
    "Given $$y\\sim {\\cal N}(w x,\\sigma^2)$$ the conjugate prior is $$w\\sim {\\cal N}(w^*,\\sigma^*)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 (5% credit). \n",
    "Which prior in the Bayesian linear regression framework leads to: a) Ridge regression, b) Lasso regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) $w_j\\sim{\\cal N}(0,\\sigma/\\sqrt{\\lambda})$\n",
    "\n",
    "b) $p(w_j)\\sim e^{-\\lambda|w_j|/\\sigma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 (10% credit). \n",
    "Answer some questions about the properties of Lasso/Ridge:\n",
    "\n",
    "a) Which of the Lasso and Ridge regressions possess an analytic solution in the closed form?\n",
    "\n",
    "b) Which of the Lasso and Ridge regressions often use to completely eliminate coefficients for some of the regressors?\n",
    "\n",
    "c) Which value of $\\lambda$ makes Lasso and Ridge identical to the ordinary least square regression?\n",
    "\n",
    "d) What dataset (training, validation or test) you would use to estimate the $\\lambda$ for Lasso or Ridge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Ridge\n",
    "\n",
    "b) Lasso\n",
    "\n",
    "c) They are identical to OLS at $\\lambda=0$ or $\\alpha=+\\infty$\n",
    "\n",
    "d) Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 (15% credit). \n",
    "Consider a Bayesian univariate linear regression $y\\sim {\\cal N}(wx,\\sigma^2)$ with a given $\\sigma=1$ and the prior $w\\sim {\\cal N}(5,1)$. Compute the posterior distribution after an observation $y=12,x=2$. After all, what is the probability of having $w<5$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 (20% credit). \n",
    "For the training and test sets provided below, train a linear regression of $y$ (last column) vs the rest of the columns of the table treated as regressors (intercept excluded) over the training set, apply it to the test set and report it's R2 performance over both - training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_train=pd.read_csv(\"Homework1_training.csv\")\n",
    "data_test=pd.read_csv(\"Homework1_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = data_train.iloc[:,1:41]\n",
    "train_y = data_train.iloc[:,41]\n",
    "test_x = data_test.iloc[:,1:41]\n",
    "test_y = data_test.iloc[:,41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.stats.api import ols\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.io.data as web\n",
    "import Quandl\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import pymc3 as pm\n",
    "from numpy import random\n",
    "import random as rnd\n",
    "from scipy import stats\n",
    "from pandas.stats.api import ols\n",
    "import pylab\n",
    "from sklearn import linear_model\n",
    "plt.rcParams[\"figure.figsize\"]=(10.0,8.0)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = ols(x=train_x, y=train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_y  = result.predict(x=test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error=test_y-predicted_y\n",
    "R2 = 1-error.var()/test_y.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sample R2 = 0.999999858894\n",
      "Out of sample R2 = -2.40063318779\n"
     ]
    }
   ],
   "source": [
    "print \"In sample R2 = %s\" % result.r2\n",
    "print \"Out of sample R2 = %s\" % R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6 (20% credit). \n",
    "Train Lasso and Ridge regressions using the training sample above with $\\lambda_{Ridge}=40000$ and $\\lambda_{Lasso}=40$ and report their performance over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge:\n",
      "The R-squared we found for In Sample is: 0.853278042856\n",
      "The R-squared we found for Out of Sample is: 0.656284514237\n",
      "\n",
      "Lasso:\n",
      "The R-squared we found for In Sample is: 0.869183660919\n",
      "The R-squared we found for Out of Sample is: 0.651017455205\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "ridge = linear_model.Ridge(fit_intercept=True,alpha=40000)\n",
    "\n",
    "# Lasso\n",
    "lasso = linear_model.Lasso(fit_intercept=True,alpha=40)\n",
    "\n",
    "for name, model in [('Ridge', ridge), ('Lasso', lasso)]:\n",
    "    print \"\\n%s:\" % name\n",
    "    \n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    for label, x, y in [('In Sample', train_x, train_y), ('Out of Sample', test_x, test_y)]:\n",
    "\n",
    "        pred = model.predict(x)\n",
    "\n",
    "        err = pred - y\n",
    "        R2 = 1 - np.var(err)/np.var(y)\n",
    "        print \"The R-squared we found for %s is: %s\" % (label, R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7 (25% credit). \n",
    "For each $m=1,2,...39$ train OLS, Lasso and Ridge regressions using the training sample above with $\\lambda_{Ridge}=40000$ and $\\lambda_{Lasso}=40$ using the first $m$ columns of the data tables as the regressors and plot the in-sample (training) and out-of-sample (test) R2 for all three models on the same graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracredit (25% of credit to be applied towards this or any other homework)\n",
    "Using a series of 10 random splits (cross-validation) of the training sample into approximately 60% training and 40% validation samples perform the selection of the optimal $\\lambda$ for Ridge and Lasso regression: for each $\\lambda$ of the considered sequence compute an average validation R2 for the 10 splits considered and finally select the $\\lambda$ having the highest value for this average R2. Visualize the dependence of the said average cross-validation R2 over $\\lambda$ for Lasso and Ridge. Report the R2 computed over the test set for the Lasso and Ridge trained over the entire training set with the selected optimal values of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
